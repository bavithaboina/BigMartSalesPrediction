{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6bbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce86c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv(\"train_FE.csv\")\n",
    "test_orig = pd.read_csv(\"test_FE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a63c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_orig.copy()\n",
    "test = test_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2520d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_df(predicted_test):\n",
    "    submission = test_orig[[\"Item_Identifier\",\"Outlet_Identifier\"]]\n",
    "    submission[\"Item_Outlet_Sales\"]=predicted_test\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226433cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def apply_model(model,train,test,target_feature,k_splits):\n",
    "   \n",
    "    #define predictor and response variables\n",
    "    x_train=train.drop([target_feature],axis=1)\n",
    "    y_train = train[[target_feature]]\n",
    "\n",
    "    model.fit(x_train,y_train)\n",
    "    y_train_pred = model.predict(x_train)\n",
    "   \n",
    "    #define cross-validation method to use\n",
    "    cv = KFold(n_splits=k_splits, random_state=1, shuffle=True) \n",
    "    #get the mean square error cv scores\n",
    "    scores = cross_val_score(model, x_train, y_train, scoring='neg_mean_squared_error',cv=cv, n_jobs=-1)\n",
    "    # get the square root of mean square error cv scores\n",
    "    scores = np.sqrt(np.abs(scores))\n",
    "    \n",
    "    print(f\"Training RMSE {np.sqrt(mean_squared_error(y_train,y_train_pred))}\")\n",
    "    print(f\"CV  mean = {np.mean(scores)} max = {np.max(scores)} min = {np.min(scores)} std = {np.std(scores)}\")\n",
    "    \n",
    "    y_test_pred = model.predict(test)\n",
    "   \n",
    "    print(\"Number of negative values predicted for training: {},test :{}\".format((y_train_pred<0).sum(),(y_test_pred<0).sum()))\n",
    "    \n",
    "    final_pred_df = submission_df(y_test_pred)\n",
    "   \n",
    "    return final_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fdf4e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb\n",
      "Training RMSE 711.9920295518928\n",
      "CV  mean = 1159.60877605948 max = 1199.9321617531295 min = 1088.8961589276746 std = 38.40269487895007\n",
      "Number of negative values predicted for training: 11,test :46\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_df = train.drop([\"Item_Identifier\",\"Outlet_Identifier\"],axis=1)\n",
    "test_df = test.drop([\"Item_Identifier\",\"Outlet_Identifier\"],axis=1)\n",
    "\n",
    "models={\"xgb\":XGBRegressor(random_state=1)}\n",
    "df={\"xgb\":None}\n",
    "\n",
    "for model in models.keys():\n",
    "    print(model)\n",
    "   \n",
    "    df[model] = apply_model(models[model],train_df,test_df,\"Item_Outlet_Sales\",5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88358cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "class Tuning():\n",
    "    def random_search_cv(self,model,params,train,test,target_feature):\n",
    "        \n",
    "        #define predictor and response variables\n",
    "        x_train=train.drop([target_feature],axis=1)\n",
    "        y_train = train[[target_feature]]\n",
    "        \n",
    "        randomcv=RandomizedSearchCV(estimator = model,param_distributions = params,scoring= 'neg_root_mean_squared_error',n_iter = 100,cv = 3,verbose = 2,\n",
    "                               random_state=100,n_jobs=-1)\n",
    "        randomcv.fit(x_train,y_train)\n",
    "        print(randomcv.best_params_)\n",
    "        \n",
    "        #best_model = randomcv.best_estimator_\n",
    "        #best_model.fit(x_train,y_train)\n",
    "        #y_pred_test = best_model\n",
    "        print(\"best random score\" ,randomcv.best_score_)\n",
    "        return randomcv.best_estimator_\n",
    "    \n",
    "    \n",
    "    \n",
    "    def apply_model(self,model,train,test,target_feature,k_splits):\n",
    "   \n",
    "        #define predictor and response variables\n",
    "        x_train=train.drop([target_feature],axis=1)\n",
    "        y_train = train[[target_feature]]\n",
    "\n",
    "        model.fit(x_train,y_train)\n",
    "        y_train_pred = model.predict(x_train)\n",
    "   \n",
    "        #define cross-validation method to use\n",
    "        cv = KFold(n_splits=k_splits, random_state=1, shuffle=True) \n",
    "        #get the mean square error cv scores\n",
    "        scores = cross_val_score(model, x_train, y_train, scoring='neg_mean_squared_error',cv=cv, n_jobs=-1)\n",
    "        # get the square root of mean square error cv scores\n",
    "        scores = np.sqrt(np.abs(scores))\n",
    "    \n",
    "        print(f\"Training RMSE {np.sqrt(mean_squared_error(y_train,y_train_pred))}\")\n",
    "        print(f\"CV  mean = {np.mean(scores)} max = {np.max(scores)} min = {np.min(scores)} std = {np.std(scores)}\")\n",
    "    \n",
    "        y_test_pred = model.predict(test)\n",
    "   \n",
    "        print(\"Number of negative values predicted for training: {},test :{}\".format((y_train_pred<0).sum(),(y_test_pred<0).sum()))\n",
    "    \n",
    "        final_pred_df = submission_df(y_test_pred)\n",
    "   \n",
    "        return final_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2d53674",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\"eta\": [x for x in np.linspace(0.001,0.2,10)],\n",
    "             \"min_child_weight\" : [int(x) for x in np.linspace(1,100,10)],\n",
    "            \"max_depth\" : [int(x) for x in np.linspace(3,10,8)],\n",
    "            \"sub_sample\" : [x for x in np.linspace(0.5,1,5)],\n",
    "             \"col_sample\":[x for x in np.linspace(0.5,1,5)],\n",
    "            \"min_samples_leaf\":[int(x) for x in np.linspace(1,10,5)],\n",
    "            \"max_leaf_nodes\":[int(x) for x in np.linspace(1,10,5)],\n",
    "            \"max_samples\":[x for x in np.linspace(1,10,5)]}\n",
    "\n",
    "models_with_params = [(\"xgboost\",XGBRegressor(random_state=1),xgb_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cd6481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[22:36:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"col_sample\", \"max_leaf_nodes\", \"max_samples\", \"min_samples_leaf\", \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "{'sub_sample': 0.625, 'min_samples_leaf': 1, 'min_child_weight': 100, 'max_samples': 10.0, 'max_leaf_nodes': 7, 'max_depth': 4, 'eta': 0.045222222222222226, 'col_sample': 0.75}\n",
      "best random score -1080.908634918749\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None, col_sample=0.75,\n",
      "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "             early_stopping_rounds=None, enable_categorical=False,\n",
      "             eta=0.045222222222222226, eval_metric=None, gamma=0, gpu_id=-1,\n",
      "             grow_policy='depthwise', importance_type=None,\n",
      "             interaction_constraints='', learning_rate=0.0452222228,\n",
      "             max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=4,\n",
      "             max_leaf_nodes=7, max_leaves=0, max_samples=10.0,\n",
      "             min_child_weight=100, min_samples_leaf=1, missing=nan,\n",
      "             monotone_constraints='()', n_estimators=100, n_jobs=0, ...)\n",
      "[22:36:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"col_sample\", \"max_leaf_nodes\", \"max_samples\", \"min_samples_leaf\", \"sub_sample\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "Training RMSE 1053.908882394424\n",
      "CV  mean = 1075.4829064320502 max = 1109.691573484424 min = 1006.4778932220254 std = 35.7114111045489\n",
      "Number of negative values predicted for training: 0,test :0\n"
     ]
    }
   ],
   "source": [
    "train_df = train.drop([\"Item_Identifier\",\"Outlet_Identifier\"],axis=1)\n",
    "test_df = test.drop([\"Item_Identifier\",\"Outlet_Identifier\"],axis=1)\n",
    "\n",
    "for model in models_with_params:\n",
    "    print(model[0])\n",
    "    tuning_obj = Tuning()\n",
    "    best_estimator = tuning_obj.random_search_cv(model[1],model[2],train_df,test_df,\"Item_Outlet_Sales\")\n",
    "    print(best_estimator)\n",
    "    df[\"XGBWithTuning\"]=tuning_obj.apply_model(best_estimator,train_df,test_df,\"Item_Outlet_Sales\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee1a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"XGBWithTuning\"].to_csv(\"XGBTuning.csv\",index=False)\n",
    "#Your score for this submission is : 1154.1352887420287."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb764489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
